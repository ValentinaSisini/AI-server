# AI-server
Questo repository documenta tutti i passaggi necessari per deployare un modello LLM su un server, dalla configurazione dell’ambiente alla gestione dell’inferenza. Include script, istruzioni e note tecniche per ogni fase: installazione delle dipendenze, preparazione del modello, ottimizzazione delle prestazioni e avvio del servizio.
